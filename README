Performance details of acn/ and even_faster_acn/
Base acn is pretty fast already, vectorized the distance calcs over https://github.com/johannah/ACN, which uses a simple on GPU knn setup using torch.topk, compared to https://github.com/jalexvig/associative_compression_networks which used sklearn KNeighbors on CPU

Timing comparison between acn and even faster acn, same model size (512 hidden, see code for model and prior arch), same machine, same V100 GPU, pytorch 1.10.1+cu102 , on the MNIST dataset. Training losses seem similar up to randomness (seeds aren't set here)

acn/
====> Epoch: 99 Average loss: 84.3620
Total training time 896.0809817314148 seconds
Dataset size in examples 60000
Number of epochs 100
Total train steps taken in batches 46900
Average time per batch 0.019106204301309482 seconds

even_faster_acn/
====> Epoch: 99 Average loss: 84.4206
Total training time 556.3746314048767 seconds
Dataset size in examples 60000
Number of epochs 100
Total train steps taken in batches 46900
Average time per batch 0.011862998537417414 seconds

Check the kneighbors functions on PriorModel() in each folder's acn_models.py for the changes
